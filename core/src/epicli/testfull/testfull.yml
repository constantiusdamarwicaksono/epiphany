kind: epiphany-cluster
title: Epiphany cluster Config
provider: any
name: default
specification:
  name: testfull
  admin_user:
    name: operations
    key_path: /user/.ssh/epiphany-operations/id_rsa
  components:
    kubernetes_master:
      count: 1
      machines:
      - default-k8s-master
    kubernetes_node:
      count: 2
      machines:
      - default-k8s-node1
      - default-k8s-node2
    logging:
      count: 1
      machines:
      - default-logging
    monitoring:
      count: 1
      machines:
      - default-monitoring
    kafka:
      count: 2
      machines:
      - default-kafka1
      - default-kafka2
    postgresql:
      count: 1
      machines:
      - default-postgresql
    load_balancer:
      count: 1
      machines:
      - default-loadbalancer
    rabbitmq:
      count: 1
      machines:
      - default-rabbitmq
version: 0.5.2
---
kind: configuration/feature-mapping
title: Feature mapping to roles
name: default
specification:
  available_roles:
  - name: repository
    enabled: yes
  - name: firewall
    enabled: yes
  - name: image-registry
    enabled: yes
  - name: kubernetes-master
    enabled: yes
  - name: kubernetes-node
    enabled: yes
  - name: logging
    enabled: yes
  - name: opendistro-for-elasticsearch
    enabled: yes
  - name: elasticsearch
    enabled: yes
  - name: elasticsearch-curator
    enabled: yes
  - name: kibana
    enabled: yes
  - name: filebeat
    enabled: yes
  - name: prometheus
    enabled: yes
  - name: grafana
    enabled: yes
  - name: node-exporter
    enabled: yes
  - name: jmx-exporter
    enabled: yes
  - name: zookeeper
    enabled: yes
  - name: kafka
    enabled: yes
  - name: rabbitmq
    enabled: yes
  - name: kafka-exporter
    enabled: yes
  - name: postgresql
    enabled: yes
  - name: haproxy
    enabled: yes
  - name: haproxy-exporter
    enabled: yes
  - name: applications
    enabled: yes
  - name: ignite
    enabled: yes

  roles_mapping:
    kafka:
    - zookeeper
    - jmx-exporter
    - kafka
    - kafka-exporter
    - node-exporter
    - filebeat
    - firewall
    rabbitmq:
    - rabbitmq
    - node-exporter
    - filebeat
    - firewall
    logging:
    - logging
    - kibana
    - filebeat
    - firewall
    load_balancer:
    - haproxy
    - haproxy-exporter
    - node-exporter
    - filebeat
    - firewall
    monitoring:
    - prometheus
    - grafana
    - node-exporter
    - filebeat
    - firewall
    postgresql:
    - postgresql
    - node-exporter
    - filebeat
    - firewall
    custom:
    - repository
    - image-registry
    - kubernetes-master
    - node-exporter
    - filebeat
    - rabbitmq
    - postgresql
    - prometheus
    - grafana
    - node-exporter
    - logging
    - firewall
    single_machine:
    - repository
    - image-registry
    - kubernetes-master
    - applications
    - rabbitmq
    - postgresql
    - firewall
    kubernetes_master:
    - image-registry
    - kubernetes-master
    - repository
    - applications
    - node-exporter
    - filebeat
    - firewall
    kubernetes_node_ha:
    - kubernetes-node
    - node-exporter
    - filebeat
    - firewall
    kubernetes_node:
    - kubernetes-node
    - node-exporter
    - filebeat
    - firewall
    ignite:
    - ignite
    - node-exporter
    - filebeat
    - firewall
    opendistro_for_elasticsearch:
    - opendistro-for-elasticsearch
    - node-exporter
    - filebeat
    - firewall

version: 0.5.2
provider: any
---
kind: configuration/shared-config
title: Shared configuration that will be visible to all roles
name: default
specification:
  custom_repository_url: ''
  custom_image_registry_address: ''
  download_directory: /tmp
  vault_location: ''
  architecture_map:
    i386: '386'
    x86_64: amd64
    aarch64: arm64
    armv7l: armv7
    armv6l: armv6
  supported_os:
  - name: RedHat
    comparison_operator: ==
    version: 7.6
  - name: RedHat
    comparison_operator: ==
    version: 7.7
  - name: RedHat
    comparison_operator: ==
    version: 7.8
  - name: RedHat
    comparison_operator: ==
    version: 7.9
  - name: CentOS
    comparison_operator: ==
    version: 7.6
  - name: CentOS
    comparison_operator: ==
    version: 7.7
  - name: CentOS
    comparison_operator: ==
    version: 7.8
  - name: CentOS
    comparison_operator: ==
    version: 7.9
  - name: Ubuntu
    comparison_operator: ==
    version: 18.04
version: 0.5.2
provider: any
---
kind: configuration/image-registry
title: Epiphany image registry
name: default
specification:
  description: Local registry with Docker images
  registry_image:
    name: registry:2
    file_name: registry-2.tar
  images_to_load:
    # K8s
  - name: k8s.gcr.io/kube-apiserver:v1.14.6
    file_name: kube-apiserver-v1.14.6.tar
  - name: k8s.gcr.io/kube-controller-manager:v1.14.6
    file_name: kube-controller-manager-v1.14.6.tar
  - name: k8s.gcr.io/kube-scheduler:v1.14.6
    file_name: kube-scheduler-v1.14.6.tar
  - name: k8s.gcr.io/kube-proxy:v1.14.6
    file_name: kube-proxy-v1.14.6.tar
  - name: k8s.gcr.io/pause:3.1
    file_name: pause-3.1.tar
  - name: k8s.gcr.io/etcd:3.3.10
    file_name: etcd-3.3.10.tar
  - name: k8s.gcr.io/coredns:1.3.1
    file_name: coredns-1.3.1.tar
  - name: coredns/coredns:1.5.0
    file_name: coredns-1.5.0.tar
  - name: quay.io/coreos/flannel:v0.11.0-amd64
    file_name: flannel-v0.11.0-amd64.tar
  - name: quay.io/coreos/flannel:v0.11.0
    file_name: flannel-v0.11.0.tar
  - name: calico/node:v3.8.1
    file_name: node-v3.8.1.tar
  - name: calico/pod2daemon-flexvol:v3.8.1
    file_name: pod2daemon-flexvol-v3.8.1.tar
  - name: kubernetesui/dashboard:v2.0.0-beta1
    file_name: dashboard-v2.0.0-beta1.tar
  - name: kubernetesui/metrics-scraper:v1.0.0
    file_name: metrics-scraper-v1.0.0.tar
  - name: calico/cni:v3.8.1
    file_name: cni-v3.8.1.tar
  - name: calico/kube-controllers:v3.8.1
    file_name: kube-controllers-v3.8.1.tar
    # applications
  - name: jboss/keycloak:4.8.3.Final
    file_name: keycloak-4.8.3.Final.tar
  - name: rabbitmq:3.7.10
    file_name: rabbitmq-3.7.10.tar
  - name: apacheignite/ignite:2.5.0
    file_name: ignite-2.5.0.tar
    # K8s upgrade
    ## v1.11.5
  - name: coredns/coredns:1.1.3
    file_name: coredns-1.1.3.tar
    ## v1.12.10
  - name: k8s.gcr.io/coredns:1.2.2
    file_name: coredns-1.2.2.tar
  - name: k8s.gcr.io/etcd:3.2.24
    file_name: etcd-3.2.24.tar
  - name: k8s.gcr.io/kube-apiserver:v1.12.10
    file_name: kube-apiserver-v1.12.10.tar
  - name: k8s.gcr.io/kube-controller-manager:v1.12.10
    file_name: kube-controller-manager-v1.12.10.tar
  - name: k8s.gcr.io/kube-proxy:v1.12.10
    file_name: kube-proxy-v1.12.10.tar
  - name: k8s.gcr.io/kube-scheduler:v1.12.10
    file_name: kube-scheduler-v1.12.10.tar
    ## v1.13.12
  - name: k8s.gcr.io/coredns:1.2.6
    file_name: coredns-1.2.6.tar
  - name: k8s.gcr.io/etcd:3.2.24
    file_name: etcd-3.2.24.tar
  - name: k8s.gcr.io/kube-apiserver:v1.13.12
    file_name: kube-apiserver-v1.13.12.tar
  - name: k8s.gcr.io/kube-controller-manager:v1.13.12
    file_name: kube-controller-manager-v1.13.12.tar
  - name: k8s.gcr.io/kube-proxy:v1.13.12
    file_name: kube-proxy-v1.13.12.tar
  - name: k8s.gcr.io/kube-scheduler:v1.13.12
    file_name: kube-scheduler-v1.13.12.tar
version: 0.5.2
provider: any
---
kind: configuration/kubernetes-master
title: Kubernetes Master Config
name: default
specification:
  version: 1.14.6
  allow_pods_on_master: false
  storage:
    name: epiphany-cluster-volume
    path: /
    enable: true
    capacity: 50
    data: {}
  advanced:
    api_server_args:
      profiling: false
      enable-admission-plugins: AlwaysPullImages,DenyEscalatingExec,NamespaceLifecycle,ServiceAccount,NodeRestriction
      audit-log-path: /var/log/apiserver/audit.log
      audit-log-maxbackup: 10
      audit-log-maxsize: 200
    controller_manager_args:
      profiling: false
      terminated-pod-gc-threshold: 200
    scheduler_args:
      profiling: false
    networking:
      dnsDomain: cluster.local
      podSubnet: 10.244.0.0/16
      serviceSubnet: 10.96.0.0/12
      plugin: flannel
    imageRepository: k8s.gcr.io
    certificatesDir: /etc/kubernetes/pki
    etcd_args:
      encrypted: yes
version: 0.5.2
provider: any
---
kind: configuration/repository
title: Epiphany requirements repository
name: default
specification:
  description: Local repository of binaries required to install Epiphany
  download_done_flag_expire_minutes: 120
version: 0.5.2
provider: any
---
kind: configuration/applications
title: Kubernetes Applications Config
name: default
specification:
  applications:
  - name: rabbitmq 2
    image_path: rabbitmq:3.7.10
    use_local_image_registry: true
    #image_pull_secret_name: regcred # optional
    service:
      name: rabbitmq-cluster
      port: 30672
      management_port: 31672
      replicas: 2
      namespace: queue
    rabbitmq:
      #amqp_port: 5672 #optional - default 5672
      plugins: # optional list of RabbitMQ plugins
      - rabbitmq_management
      - rabbitmq_management_agent
      policies: # optional list of RabbitMQ policies
      - name: ha-policy2
        pattern: .*
        definitions:
          ha-mode: all
      custom_configurations: #optional list of RabbitMQ configurations (new format -> https://www.rabbitmq.com/configure.html)
      - name: vm_memory_high_watermark.relative
        value: 0.5
      cluster:
        #is_clustered: true #redundant in in-Kubernetes installation, it will always be clustered
        #cookie: "cookieSetFromDataYaml" #optional - default value will be random generated string


  - name: auth-service # this service require postgresql to be installed in cluster
    image_path: jboss/keycloak:4.8.3.Final
    use_local_image_registry: true
    #image_pull_secret_name: regcred
    service:
      name: as-testauthdb
      port: 30104
      replicas: 2
      namespace: namespace-for-auth
      admin_user: auth-service-username
      admin_password: auth-service-password
    database:
      name: auth-database-name
      #port: "5432" # leave it when default
      user: auth-db-user
      password: auth-db-password

version: 0.5.2
provider: any
---
kind: configuration/node-exporter
title: Node exporter
name: default
specification:
  description: Service that runs Prometheus Node Exporter
  file_name: node_exporter-0.16.0.linux-amd64.tar.gz
  enabled_collectors:
  - conntrack
  - diskstats
  - entropy
  - filefd
  - filesystem
  - loadavg
  - mdadm
  - meminfo
  - netdev
  - netstat
  - sockstat
  - stat
  - textfile
  - time
  - uname
  - vmstat
  - systemd

  web_listen_port: '9100'
  web_listen_address: ''
  collector_netdev_ignored_devices: ^$
  config_flags:
  - --web.listen-address=:9100
  - --log.level=info
  - --collector.diskstats.ignored-devices=^(ram|loop|fd)\d+$
  - --collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)
  - --collector.netdev.ignored-devices="^$"
  - --collector.textfile.directory="/var/lib/prometheus/node-exporter"
  - --collector.systemd.unit-whitelist="(kafka\.service|zookeeper\.service)"

  config_for_prometheus:
    exporter_listen_port: '9100'
    prometheus_config_dir: /etc/prometheus
    file_sd_labels:
    - label: job
      value: node
version: 0.5.2
provider: any
---
kind: configuration/filebeat
title: Filebeat
name: default
specification:
  filebeat_version: 6.8.5
version: 0.5.2
provider: any
---
kind: configuration/firewall
title: OS level firewall
name: default
specification:
  Debian:
    install_firewalld: false
  firewall_service_enabled: false
  apply_configuration: false
  managed_zone_name: epiphany
  rules:
    applications:
      enabled: false
      ports:
      - 30104/tcp         # auth-service
      - 30672/tcp         # rabbitmq-amqp
      - 31672/tcp         # rabbitmq-http (management)
      - 32300-32302/tcp   # ignite
    common:
      enabled: true
      ports:
      - 22/tcp
    elasticsearch:
      enabled: true
      ports:
      - 9200/tcp
    grafana:
      enabled: true
      ports:
      - 3000/tcp
    haproxy:
      enabled: true
      ports:
      - 443/tcp
      - 9000/tcp   # stats
    haproxy_exporter:
      enabled: true
      ports:
      - 9101/tcp
    ignite:
      enabled: true
      ports:
      - 8080/tcp    # REST API
      - 10800/tcp   # thin client connection
      - 11211/tcp   # JDBC
      - 47100/tcp   # local communication
      - 47500/tcp   # local discovery
    image_registry:
      enabled: true
      ports:
      - 5000/tcp
    jmx_exporter:
      enabled: true
      ports:
      - 7071/tcp   # Kafka
      - 7072/tcp   # ZooKeeper
    kafka:
      enabled: true
      ports:
      - 9092/tcp
      # - 9093/tcp # encrypted communication (if TLS/SSL is enabled)
    kafka_exporter:
      enabled: true
      ports:
      - 9308/tcp
    kibana:
      enabled: true
      ports:
      - 5601/tcp
    kubernetes_master:
      enabled: true
      ports:
      - 6443/tcp        # API server
      - 2379-2380/tcp   # etcd server client API
      - 8472/udp        # flannel (vxlan backend)
      - 10250/tcp       # Kubelet API
      - 10251/tcp       # kube-scheduler
      - 10252/tcp       # kube-controller-manager
    kubernetes_node:
      enabled: true
      ports:
      - 8472/udp    # flannel (vxlan backend)
      - 10250/tcp   # Kubelet API
    logging:
      enabled: true
      ports:
      - 9200/tcp
    node_exporter:
      enabled: true
      ports:
      - 9100/tcp
    opendistro_for_elasticsearch:
      enabled: true
      ports:
      - 9200/tcp
    postgresql:
      enabled: true
      ports:
      - 5432/tcp
      - 6432/tcp   #PGBouncer
    prometheus:
      enabled: true
      ports:
      - 9090/tcp
      - 9093/tcp   # Alertmanager
    rabbitmq:
      enabled: true
      ports:
      - 4369/tcp      # peer discovery service used by RabbitMQ nodes and CLI tools
        # - 5671/tcp  # encrypted communication (if TLS/SSL is enabled)
      - 5672/tcp      # AMQP
        # - 15672/tcp # HTTP API clients, management UI and rabbitmqadmin (only if the management plugin is enabled)
      - 25672/tcp     # distribution server
    zookeeper:
      enabled: true
      ports:
      - 2181/tcp   # client connections
      - 2888/tcp   # peers communication
      - 3888/tcp   # leader election
version: 0.5.2
provider: any
---
kind: configuration/kubernetes-node
title: Kubernetes Node Config
name: default
specification:
  version: 1.14.6
  node_labels: node-type=epiphany
version: 0.5.2
provider: any
---
kind: configuration/logging
title: Logging Config
name: default
specification:
  versions:
    RedHat:
      elasticsearch_oss: 7.3.2
      opendistro: 1.3.0*
    Debian:
      elasticsearch_oss: 7.3.2
      opendistro: 1.3.0*
  cluster_name: EpiphanyElastic
  clustered: true
  paths:
    data: /var/lib/elasticsearch
    logs: /var/log/elasticsearch
version: 0.5.2
provider: any
---
kind: configuration/kibana
title: Kibana
name: default
specification:
  kibana_version: 6.4.0
  kibana_log_dir: /var/log/kibana
version: 0.5.2
provider: any
---
kind: configuration/prometheus
title: Prometheus
name: default
specification:
  version: 2.10.0
  file_name: prometheus-2.10.0.linux-amd64.tar.gz
  config_directory: /etc/prometheus
  storage:
    data_directory: /var/lib/prometheus
  config_flags:                                                                   # Parameters that Prometheus service will be started with.
  - --config.file=/etc/prometheus/prometheus.yml                                  # Directory should be the same as "config_directory"
  - --storage.tsdb.path=/var/lib/prometheus                                       # Directory should be the same as "storage.data_directory"
  - --storage.tsdb.retention.time=180d                                            # Data retention time for metrics
  - --storage.tsdb.retention.size=20GB                                            # Data retention size for metrics
  - --web.console.libraries=/etc/prometheus/console_libraries                     # Directory should be the same as "config_directory"
  - --web.console.templates=/etc/prometheus/consoles                              # Directory should be the same as "config_directory"
  - --web.listen-address=0.0.0.0:9090                                             # Address that Prometheus console will be available

  metrics_path: /metrics
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 10s
  remote_write: []
  remote_read: []
  alerts:
    rules: # Predefined rules, feel free to add more
    - name: UpDown
      expression: up == 0
      duration: 1m   #1s, 1m, 1h, 1d, 1w, ...
      severity: critical
      message: Node is down.
    - name: DiskSpace
      expression: ((node_filesystem_avail_bytes* 100) / node_filesystem_size_bytes)
        < 20                                                                               # 100 - 80
      duration: 1m   #1s, 1m, 1h, 1d, 1w, ...
      severity: critical
      message: Disk usage is above 80%
    - name: DiskSpacePrediction
      expression: predict_linear(node_filesystem_free_bytes{job="node"}[1h], 48 *
        3600) < 0
      duration: 1m   #1s, 1m, 1h, 1d, 1w, ...
      severity: warning
      message: Disk will run out of space in less than 48h
    - name: MemoryUsage
      expression: (sum by (instance) (node_memory_MemTotal_bytes) - sum by (instance)(node_memory_MemFree_bytes
        + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum by (instance)(node_memory_MemTotal_bytes)
        * 100 > 80
      duration: 15m   #1s, 1m, 1h, 1d, 1w, ...
      severity: warning
      message: Server memory has been used in more than 80% during last 15 minutes.
    - name: CpuLoad
      expression: 100 - (avg by (instance) (irate(node_cpu_seconds_total{job="node",mode="idle"}[5m]))
        * 100) > 80
      duration: 15m   #1s, 1m, 1h, 1d, 1w, ...
      severity: critical
      message: CPU utilization has exceeded 80% over last 15 minutes.
    - name: KafkaConsumerLag
      expression: sum by(consumergroup) (kafka_consumergroup_lag) > 1000
      duration: 15m   #1s, 1m, 1h, 1d, 1w, ...
      severity: critical
      message: Kafka consumers are lagging more than 1000 messages over last 15 minutes.

    alertmanager:
      enable: false
      file_name: alertmanager-0.17.0.linux-amd64.tar.gz
      version: 0.17.0
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: [alertname]
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 1h
          routes:
        receivers:
version: 0.5.2
provider: any
---
kind: configuration/grafana
title: Grafana
name: default
specification:
  version: 6.2.5
  grafana_logs_dir: /var/log/grafana
  grafana_data_dir: /var/lib/grafana
  grafana_address: 0.0.0.0
  grafana_port: 3000
  grafana_use_provisioning: true
  grafana_provisioning_synced: false
  grafana_url: https://0.0.0.0:3000
  grafana_server:
    protocol: https
    enforce_domain: false
    socket: ''
    cert_key: /etc/grafana/ssl/grafana_key.key
    cert_file: /etc/grafana/ssl/grafana_cert.pem
    enable_gzip: false
    static_root_path: public
    router_logging: false
  grafana_security:
    admin_user: admin
    admin_password: admin
  grafana_database:
    type: sqlite3
  grafana_dashboards: []
  grafana_dashboards_dir: dashboards
  grafana_welcome_email_on_sign_up: false
  grafana_users:
    allow_sign_up: false
    auto_assign_org_role: Viewer
    default_theme: dark
  grafana_auth: {}
  grafana_ldap: {}
  grafana_session: {}
  grafana_analytics: {}
  grafana_smtp: {}
  grafana_alerting:
    execute_alerts: true
  grafana_log:
  grafana_metrics: {}
  grafana_tracing: {}
  grafana_snapshots: {}
  grafana_image_storage: {}
  grafana_plugins: []
  grafana_alert_notifications: []
  grafana_datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://localhost:9090
    basicAuth: false
    basicAuthUser: ''
    basicAuthPassword: ''
    isDefault: true
    editable: true
    jsonData:
      tlsAuth: false
      tlsAuthWithCACert: false
      tlsSkipVerify: true

  # API keys to configure
  grafana_api_keys: []
  grafana_logging:
    log_rotate: true
    daily_rotate: true
    max_days: 7
version: 0.5.2
provider: any
---
kind: configuration/zookeeper
title: Zookeeper
name: default
specification:
  version: 3.4.12
  file_name: zookeeper-3.4.12.tar.gz
version: 0.5.2
provider: any
---
kind: configuration/jmx-exporter
title: JMX exporter
name: default
specification:
  file_name: jmx_prometheus_javaagent-0.12.0.jar
  jmx_path: /opt/jmx-exporter/jmx_prometheus_javaagent.jar
  jmx_jars_directory: /opt/jmx-exporter/jars
  jmx_exporter_user: jmx-exporter
  jmx_exporter_group: jmx-exporter
version: 0.5.2
provider: any
---
kind: configuration/kafka
title: Kafka
name: default
specification:
  kafka_var:
    version: 2.3.1
    scala:
      version: 2.12
    file_name: kafka_2.12-2.3.1.tgz
    enabled: true
    admin: kafka
    admin_pwd: epiphany
    security:
      ssl:
        enabled: false
        port: 9093
        server:
          local_cert_download_path: kafka-certs
          keystore_location: /var/private/ssl/kafka.server.keystore.jks
          truststore_location: /var/private/ssl/kafka.server.truststore.jks
          cert_validity: 365
          passwords:
            keystore: PasswordToChange
            truststore: PasswordToChange
            key: PasswordToChange
        endpoint_identification_algorithm: HTTPS
        client_auth: required
      encrypt_at_rest: false
      inter_broker_protocol: PLAINTEXT
      authorization:
        enabled: false
        authorizer_class_name: kafka.security.auth.SimpleAclAuthorizer
        allow_everyone_if_no_acl_found: false
        super_users:
        - tester01
        - tester02
        users:
        - name: test_user
          topic: test_topic

      authentication:
        enabled: false
        authentication_method: certificates
        sasl_mechanism_inter_broker_protocol:
        sasl_enabled_mechanisms: PLAIN
    sha: b28e81705e30528f1abb6766e22dfe9dae50b1e1e93330c880928ff7a08e6b38ee71cbfc96ec14369b2dfd24293938702cab422173c8e01955a9d1746ae43f98
    port: 9092
    replicas: 1
    partitions: 8
    log_retention_hours: 168
    offset_retention_minutes: 10080
    heap_opts: -Xmx2G -Xms2G
    opts: -Djavax.net.debug=all
    jmx_opts:
    group: kafka
    user: kafka
    conf_dir: /opt/kafka/config
    data_dir: /var/lib/kafka
    log_dir: /var/log/kafka
  zookeeper_set_acl: false
  zookeeper_hosts: "{{ groups['zookeeper']|join(':2181,') }}:2181"
  jmx_exporter_user: jmx-exporter
  jmx_exporter_group: jmx-exporter
  prometheus_jmx_path: /opt/jmx-exporter/jmx_prometheus_javaagent.jar
  prometheus_jmx_exporter_web_listen_port: 7071
  prometheus_jmx_config: /opt/kafka/config/jmx-kafka.config.yml
  prometheus_config_dir: /etc/prometheus
  prometheus_kafka_jmx_file_sd_labels:
    job: jmx-kafka
version: 0.5.2
provider: any
---
kind: configuration/kafka-exporter
title: Kafka exporter
name: default
specification:
  description: Service that runs Kafka Exporter
  version: 1.2.0
  file_name: kafka_exporter-1.2.0.linux-amd64.tar.gz
  web_listen_port: '9308'
  config_flags:
  - --web.listen-address=:9308     # Address to listen on for web interface and telemetry.
  - --web.telemetry-path=/metrics     # Path under which to expose metrics.
  - --log.level=info
  - --topic.filter=.*     # Regex that determines which topics to collect.
  - --group.filter=.*     # Regex that determines which consumer groups to collect.
    #- '--tls.insecure-skip-tls-verify' # If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure.
  - --kafka.version=2.0.0
    #- '--sasl.enabled' # Connect using SASL/PLAIN.
    #- '--sasl.handshake' # Only set this to false if using a non-Kafka SASL proxy
    #- '--sasl.username=""'
    #- '--sasl.password=""'
    #- '--tls.enabled' # Connect using TLS
    #- '--tls.ca-file=""' # The optional certificate authority file for TLS client authentication
    #- '--tls.cert-file=""' # The optional certificate file for client authentication
    #- '--tls.key-file=""' # The optional key file for client authentication

  config_for_prometheus:
    exporter_listen_port: '9308'
    prometheus_config_dir: /etc/prometheus
    file_sd_labels:
    - label: job
      value: kafka-exporter
version: 0.5.2
provider: any
---
kind: configuration/postgresql
title: PostgreSQL
name: default
specification:
  replication:
    enabled: no
  config_file:
    parameter_groups:
    - name: CONNECTIONS AND AUTHENTICATION
      subgroups:
      - name: Connection Settings
        parameters:
        - name: listen_addresses
          value: "'*'"
          comment: listen on all addresses
      - name: Security and Authentication
        parameters:
        - name: ssl
          value: off
          comment: to have the default value also on Ubuntu
    - name: RESOURCE USAGE (except WAL)
      subgroups:
      - name: Kernel Resource Usage
        parameters:
        - name: shared_preload_libraries
          comment: set by automation
    - name: ERROR REPORTING AND LOGGING
      subgroups:
      - name: Where to Log
        parameters:
        - name: log_directory
          value: "'/var/log/postgresql'"
          comment: to have standard location for Filebeat and logrotate
        - name: log_filename
          value: "'postgresql.log'"
          comment: to use logrotate with common configuration
  additional_components:
    pgbouncer:
      enabled: no
  extensions:
    pgaudit:
      enabled: no
      shared_preload_libraries:
      - pgaudit
      config_file_parameters:
        log_connections: off
        log_disconnections: off
        log_statement: none
        log_line_prefix: "'%m [%p] %q%u@%d,host=%h '"
        pgaudit.log: "'write, function, role, ddl' # 'misc_set' is not supported for\
          \ PG 10"
        pgaudit.log_catalog: 'off # to reduce overhead of logging'
        pgaudit.log_relation: 'on # separate log entry for each relation'
        pgaudit.log_statement_once: off
        pgaudit.log_parameter: on
  logrotate:
    config: |-
      /var/log/postgresql/postgresql*.log {
          maxsize 10M
          daily
          rotate 6
          copytruncate
      # delaycompress is for Filebeat
          delaycompress
          compress
          notifempty
          missingok
          su root root
          nomail
      # to have multiple unique filenames per day when dateext option is set
          dateformat -%Y%m%dH%H
      }
version: 0.5.2
provider: any
---
kind: configuration/haproxy
title: HAProxy
name: default
specification:
  version: '1.8'
  service_port: 30001
  logs_max_days: 60
  self_signed_certificate_name: self-signed-fullchain.pem
  self_signed_private_key_name: self-signed-privkey.pem
  self_signed_concatenated_cert_name: self-signed-test.tld.pem
  haproxy_log_path: /var/log/haproxy.log
  stats:
    enable: true
    bind_address: 127.0.0.1:9000
    uri: /haproxy?stats
    user: operations
    password: your-haproxy-stats-pwd
  frontend:
  - name: https_front
    port: 443
    https: yes
    backend:
    - http_back1
  backend: # example backend config below
  - name: http_back1
    server_groups:
    - kubernetes_node
      # servers: # Definition for server to that hosts the application.
      # - name: "node1"
      #   address: "epiphany-vm1.domain.com"
    port: 30104
version: 0.5.2
provider: any
---
kind: configuration/haproxy-exporter
title: HAProxy exporter
name: default
specification:
  version: 0.10.0
  file_name: haproxy_exporter-0.10.0.linux-amd64.tar.gz
  description: Service that runs HAProxy Exporter
  web_listen_port: '9101'
  config_for_prometheus:
    exporter_listen_port: '9101'
    prometheus_config_dir: /etc/prometheus
    file_sd_labels:
    - label: job
      value: haproxy-exporter
version: 0.5.2
provider: any
---
kind: configuration/rabbitmq
title: RabbitMQ
name: default
specification:
  version: 3.7.10
  rabbitmq_user: rabbitmq
  rabbitmq_group: rabbitmq
  logrotate_period: weekly
  logrotate_number: 10
  ulimit_open_files: 65535
  amqp_port: 5672
  rabbitmq_use_longname: true
  rabbitmq_policies: []
  rabbitmq_plugins: []
  custom_configurations: []
  cluster:
    is_clustered: false
version: 0.5.2
provider: any
---
kind: infrastructure/machine
provider: any
name: default-k8s-master
specification:
  hostname: master
  ip: 192.168.100.101
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-k8s-node1
specification:
  hostname: node1
  ip: 192.168.100.102
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-k8s-node2
specification:
  hostname: node2
  ip: 192.168.100.103
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-logging
specification:
  hostname: elk
  ip: 192.168.100.105
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-monitoring
specification:
  hostname: prometheus
  ip: 192.168.100.106
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-kafka1
specification:
  hostname: kafka1
  ip: 192.168.100.107
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-kafka2
specification:
  hostname: kafka2
  ip: 192.168.100.108
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-postgresql
specification:
  hostname: postgresql
  ip: 192.168.100.109
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-loadbalancer
specification:
  hostname: loadbalancer
  ip: 192.168.100.110
version: 0.5.2
---
kind: infrastructure/machine
provider: any
name: default-rabbitmq
specification:
  hostname: rabbitmq
  ip: 192.168.100.111
version: 0.5.2
